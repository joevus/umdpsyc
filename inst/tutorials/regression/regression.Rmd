---
title: "Correlation and Regression"
output: learnr::tutorial
runtime: shiny_prerendered
tutorial:
  id: "regression"
  version: 0.5
---


```{r setup, include=FALSE}
library(learnr)
library(umdpsyc)
library(gradethis)
library(ggplot2)
knitr::opts_chunk$set(exercise.checker = gradethis::grade_learnr)
knitr::opts_chunk$set(echo = FALSE)
learnr::tutorial_options(
  exercise.timelimit = 60)
```

## Correlation

```{r}
data(exam_scores)
```

```{r, echo=TRUE}
qplot(exam_scores$motivation, exam_scores$exam1)
```

```{r, echo=TRUE}
qplot(exam_scores$sleep_week, exam_scores$exam1)
```


You can use the same function to find both Spearman's Rho rank correlation and Pearson's correlation. The `cor` function is used for both, and you simply specify the method as an argument in the function. The first two arguments are the variables you want to find the correlation for, while the third is the type of correlation that you want to find. 

### Spearman's Rho

```{r}
cor(exam_scores$motivation,exam_scores$exam1,method = 'spearman')
```

### Pearson Correlation

```{r}
cor(exam_scores$sleep_week,exam_scores$exam1,method = 'pearson')
```
## Regression

###

Let's fit a least squares regression line to this. 

```{r simplemodel}
exammod <- lm(exam1 ~ sleep_week, data = exam_scores)
summary(exammod)
```

Notice that the code to find the regression line is very similar to that of ANOVA. Essentially everything is the same except we use `lm()` instead of `aov()`. 

### Regression Diagnostics

To get regression diagnostic plots, we can simply use `plot()` on the model object. The first two plots show the residuals. The first one is the residual plot, which you should check to make sure there is no pattern and that there is constant variance. The second is a QQ-plot checking the normality of the residuals. The next two are used to check for outliers and influential points.

```{r}
plot(exammod)
```

### Multiple Regression

Just like we extended ANOVA to factorial ANOVA using a `*`, we can do the same to extend to multiple regression.

```{r multmodel}
exammod2 <- lm(exam1 ~ motivation * sleep_week, data = exam_scores)
summary(exammod2)
```

Notice that the interaction is significant, even though none of the main effects are significant. This does sometimes happen, and that's because the "main effect" doesn't have the same interpretation anymore in the presence of an interaction. The coefficient for each just represent the effect when the other is at 0. So, for example, `-0.002729` is the effect of `SATSum` when `HSGPA` is 0. The interaction being significant in this case is actually an indication that the effect of `SATSum` is significant for other values of `HSGPA`.


## Exercises

### Exercise 3

How would you find a linear regression model using scores on the first exam as the outcome and two predictor variables: minutes of sleep the week before and student motivation, including an interaction term? 


```{r exercise3, exercise=TRUE}

```

```{r exercise3-check}
grade_result(
  pass_if(~ identical(.result, summary(lm(exam1 ~ motivation*sleep_week, data = exam_scores))), "Good Job!"),
  fail_if(~ TRUE)
)
```

### Submitting work

```{r submission-setup}
submission_code <- function(UID, exercise = 'data-frames'){
  httr::sha1_hash('regression121',as.character(UID))
}
```

Generate your submission code by putting in your UID in the function below. For example, if your UID is `2`, then your code should look like `submission_code(UID = 2)`
```{r submission, exercise = TRUE}
submission_code(UID = )
```
